---
layout: default
---

<!-- <figure>
    <center><img src="https://offline-rl.github.io/assets/OFFLINE_RL.gif" style="width:65%"; /></center>
    <figcaption style='text-align: center'><small>Source: <a href="https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html">Google AI Blog </a></small></figcaption>
</figure> -->

<div class="row">

<!-- <p style="color:red;"> The website for <b>2<sup>nd</sup></b> offline RL workshop at NeurIPS 2021 can be found at <a href=https://offline-rl-neurips.github.io/2021> offline-rl-neurips.github.io/2021</a>.<p> -->

<h3>Summary</h3>
<div class="break"></div>

<p>This workshop is about decision awareness in RL, which we refer to as the knowledge that each of the components of an RL system should be explicitly trained to help the agent take the optimal action. A classical example of decision awareness is the recognition that a world model does not have to focus on accurately predicting all the granular variations in all parts of the environment but rather focus on elements that are important for decision making. Our goal is to give an outlet for a discussion about decision-aware RL algorithms, implications and real-world applications of these techniques, as well as in widening the understanding of the conceptual and practical limitations of existing approaches.</p>
<p>The submission deadline is May 27, 2022, Anywhere on Earth.</p>

<hr>

<p>A reinforcement learning (RL) agent typically contains several modules such as a policy, a value function, or a model of the environment's dynamics. Conventionally, each of these modules is trained to perform well with respect to a criterion that does not explicitly consider its role, in interaction with other modules, in the eventual decision making of the agent. For instance, a model in a model-based RL agent is trained to be an accurate predictor of the environment dynamics; but accurate predictions may not be feasible or even necessary, for instance, when dealing with the irrelevant details of an image in a visual-based environment. Trying to learn an accurate model of the dynamics requires a much larger number of samples and higher-capacity function approximators compared to a <span><em>decision-aware</em></span> model that only focuses on modelling the relevant aspects of the environment. Decision awareness refers to the principle that each module should be trained to <span><em>explicitly</em></span> consider how its interaction with other modules leads to improving the agentâ€™s long-term performance.</p>
<p>Decision awareness goes beyond model learning. In actor-critic algorithms, a critic is trained to predict the expected return while later used to aid policy optimization. Is the accuracy on return prediction a sensible goal for critic learning? Or can we train a critic that considers its interaction with the policy and directly maximizes the resulting performance? More generally, what is the best way to learn each components of an RL agent, considering their mutual interactions and the eventual goal of maximizing rewards? Through this workshop, we seek answers to this question.</p>
    <p>For a list of representative papers, please visit the <a href="resources.html"> resources page</a>.</p>
</div>

<!-- <div id="PC" class="row">
<h3>Program Committee</h3>
<div class="break"></div>
	<ul style="width:25%; float:left; display: inline; ">
		<li>Ajay Mandlekar</li>
		<li>Alex Irpan</li>
		<li>Amy	Zhang</li>
		<li>Ankesh	Anand	</li>
		<li>Aravind	Rajeswaran</li>
		<li>Ashvin Nair</li>
		<li>Aurick Zhou</li>
		<li>Avi	Singh </li>
		<li>Ben	London </li>
		<li>Benjamin Eysenbach</li>
		<li>Bo Dai</li>
		<li>Caglar Gulcehre</li>
		<li>Chen Tessler</li>
		<li>Cosmin Paduraru</li>
		<li>Daniel Seita</li>
	 </ul>

	<ul style="width:25%; float:center; display: inline; ">
		<li>Dibya Ghosh</liMany commented on the phenomenon. Sarah Snell Bryant, of Cummington, Massachusetts, wrote in her diary, "Weather backward."[27]
		<li>Ehsan Mehralian	</li>
		<li>Emmanuel Bengio</li>
		<li>Garrett	Thomas</li>
		<li>Hadi Nekoei	</li>
		<li>Ilya Kostrikov</li>
		<li>Jacob Buckman</li>
		<li>Jae Hyun Lim</li>
		<li>Jiawei Huang</li>
		<li>Justin Fu</li>
		<li>Kamyar Ghassemipour</li>
		<li>Masatoshi Uehara </li>
		<li>Natasha	Jaques </li>
		<li>Ofir Nachum</li>
		<li>Ramki Gummadi</li>
	</ul>

	<ul style="width:25%; float:right; display: inline;">
		<li>Riashat	Islam</li>
		<li>Romain Laroche</li>
		<li>Romina	Abachi	</li>
		<li>Shangtong Zhang</li>
		<li>Taylor Killian</li>
		<li>Tengyang Xie</li>
		<li>Tianhe Yu</li>
		<li> Xinyang Geng </li>
		<li>Xue Bin	Peng</li>
		<li>Yao	Liu	</li>
		<li>Yevgen Chebotar</li>
		<li>Yi Su</li>
		<li>Ziyu Wang </li>
	</ul>
</div> -->

<div id="organizers" class="row">
<h2 style="float:left;">Organizers</h2>
<div class="break"></div>
<div style="text-align: left;">
{%- for person in site.data.organizers -%}
<div class="person">
  <img src="{{ person.image }}" height="170px" /><div style="height:12px;"></div>
   <a href="{{ person.url | relative_url }}">{{ person.name }}</a> <div style="height:4px;"></div>
   <span>{{ person.title | replace: '&', '<div style="height:4px;"></div>' }}</span>
</div>
{%- endfor -%}
</div>
</div>
<p> To contact the organizers, please send an email to <a href="mailto:darl.workshop@gmail.com">darl.workshop@gmail.com</a>. <p>


<p style="text-align:right">
  <small> Thanks to Rishabh Agarwal for providing this <a href="https://offline-rl-neurips.github.io/">template</a>. </small> </p>
