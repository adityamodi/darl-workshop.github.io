---
layout: default
---

<!-- <figure>
    <center><img src="https://offline-rl.github.io/assets/OFFLINE_RL.gif" style="width:65%"; /></center>
    <figcaption style='text-align: center'><small>Source: <a href="https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html">Google AI Blog </a></small></figcaption>
</figure> -->

<div class="row">

<!-- <p style="color:red;"> The website for <b>2<sup>nd</sup></b> offline RL workshop at NeurIPS 2021 can be found at <a href=https://offline-rl-neurips.github.io/2021> offline-rl-neurips.github.io/2021</a>.<p> -->

<p>The goal of reinforcement learning (RL) is to maximize a reward signal by taking optimal decisions. An RL agent typically contains several modules such as a policy, a value function, and, in model-based RL, a model of the environment's dynamics. Conventionally, each of these modules is trained to perform well with respect to a criterion that does not explicitly consider their role, in interaction with other modules, in the eventual decision making of the agent. For instance, a model in a model-based RL agent is trained to be an accurate predictor of the environment dynamics, despite accurate predictions may not be feasible or even necessary. For example, one may not need to model the irrelevant details of an image in a visual-based environment; but trying to learn an accurate model of the dynamics requires a much larger number of samples and higher-capacity function approximators compared to a <span><em>decision-aware</em></span> model that only focuses on modelling the relevant aspects of the environment. Decision awareness refers to the principle that each module should be trained to <span><em>explicitly</em></span> consider how its interaction with other modules leads to improving the agent’s long-term performance, as specified by the reward signal.</p>
<p>Decision awareness goes beyond model learning. In actor-critic algorithms, a critic is trained to predict the expected return while later used to aid policy optimization. Is the accuracy on return prediction a sensible goal for critic learning? Or can we train a critic that considers its interaction with the policy and directly maximizes the resulting performance? More generally, what is the best way to learn each components of an RL agent, considering their mutual interactions and the eventual goal of maximizing rewards? Our workshop aims at answering these questions and articulating that decision awareness might be a key towards providing effective solutions to grand challenges in RL including exploration and sample efficiency.</p>
<p>Recently, there has been a rising interest in the RL community in studying and leveraging the interplay among the moving parts of RL systems. It has been done by designing new loss functions, end-to-end procedures, and meta-learning frameworks. Decision awareness lead not only to a deeper theoretical understanding of the limitations of traditional RL approaches, but also to high-performing algorithms with promising applicability to real-world problems. We believe it is now the ideal time for a first workshop to discuss and unify the ideas in the community, as well as to plan the possible future steps for this family of methods.</p>

<!-- <p>The remarkable success of deep learning has been driven by the availability of large and diverse datasets such as ImageNet. In contrast, the common paradigm in reinforcement learning (RL) assumes that an agent frequently interacts with the environment and learns using its own collected experience. This mode of operation is prohibitive for many complex real-world problems, where repeatedly collecting diverse data is expensive (<span><em>e.g.,</em></span> robotics or educational agents) and/or potentially unsafe (<span><em>e.g.,</em></span> healthcare).</p> -->
<!-- <p>Alternatively, <em>Offline RL</em> focuses on training agents with logged data with no further environment interaction. Offline RL promises to bring forward a data-driven RL paradigm and carries the potential to scale up end-to-end learning approaches to real-world decision making tasks such as robotics, recommendation systems, dialogue generation, autonomous driving, healthcare systems and safety-critical applications. Recently, successful deep RL algorithms have been adapted to the offline RL setting and demonstrated a potential for success in a number of domains, however, significant algorithmic and practical challenges remain to be addressed. Within the past two years, performance on simple benchmarks has rapidly risen, so the community has also started developing standardized benchmarks (<a href="https://deepmind.com/research/open-source/RL-Unplugged-An-Offline-RL-Benchmark-Suite">RLUnplugged</a>, <a href="https://sites.google.com/view/d4rl/home">D4RL</a>) specifically designed to stress-test offline RL algorithms.  -->
<!-- However, these are only initial proposals that could benefit from community input on design and evaluation protocol.</p> -->

<!-- <p><strong>Goal of the workshop</strong>: Our goal is to bring attention to offline RL, both from within and from outside the RL community (e.g., causal inference, optimization, self-supervised learning), discuss algorithmic challenges that need to be addressed, discuss potential real-world applications, discuss limitations and challenges, and come up with concrete problem statements and evaluation protocols, inspired from real-world applications, for the research community to work on. In particular, we are interested in bringing together researchers and practitioners to discuss questions on theoretical, empirical, and practical aspects of offline RL, including but not limited to,</p>
<ul>
<li>Algorithmic decisions and associated challenges in training RL agents offline</li>
<li>Properties of supervision needed to guarantee the success of offline RL methods</li>
<li>Relationship with learning under uncertainty: Bayesian inference and causal inference</li>
<li>Model selection, off-policy evaluation and theoretical limitations</li>
<li>Relationship and integration with the conventional online RL paradigms</li>
<li>Evaluation protocols and frameworks and real-world datasets and benchmarks</li>
<li>Connections to transfer learning, self-supervised learning and generative modelling</li>
</ul>
</p>
</div> -->

<p><strong>Goal of the workshop</strong>: Our goal is to give an outlet for a discussion about training decision-aware RL algorithms and mitigating objective mismatches of traditional approaches. We are interested in the implications and real-world applications of these techniques, as well as in widening the understanding of the conceptual and practical limitations of existing approaches. We look for theoretical, algorithmic, and empirical contributions, including, but not limited to,</p>
<ul>
<li>Decision-aware objectives, end-to-end procedures, and meta-learning techniques for training and discovering components in modular RL systems</li>
<li>Theoretical and empirical analyses of the interaction among multiple modules used by RL algorithms</li>
<li>Studies of the performance of model-based RL agents under model misspecification</li>
<li>The role of unsupervised learning and auxiliary training signals in RL and their relation to decision-awareness</li>
<li>Connections with other fields such as control theory, cognitive science, and robotics</li>
</ul>
</p>
</div>

<!-- <div id="PC" class="row">
<h3>Program Committee</h3>
<div class="break"></div>
	<ul style="width:25%; float:left; display: inline; ">
		<li>Ajay Mandlekar</li>
		<li>Alex Irpan</li>
		<li>Amy	Zhang</li>
		<li>Ankesh	Anand	</li>
		<li>Aravind	Rajeswaran</li>
		<li>Ashvin Nair</li>
		<li>Aurick Zhou</li>
		<li>Avi	Singh </li>
		<li>Ben	London </li>
		<li>Benjamin Eysenbach</li>
		<li>Bo Dai</li>
		<li>Caglar Gulcehre</li>
		<li>Chen Tessler</li>
		<li>Cosmin Paduraru</li>
		<li>Daniel Seita</li>
	 </ul>

	<ul style="width:25%; float:center; display: inline; ">
		<li>Dibya Ghosh</liMany commented on the phenomenon. Sarah Snell Bryant, of Cummington, Massachusetts, wrote in her diary, "Weather backward."[27]
		<li>Ehsan Mehralian	</li>
		<li>Emmanuel Bengio</li>
		<li>Garrett	Thomas</li>
		<li>Hadi Nekoei	</li>
		<li>Ilya Kostrikov</li>
		<li>Jacob Buckman</li>
		<li>Jae Hyun Lim</li>
		<li>Jiawei Huang</li>
		<li>Justin Fu</li>
		<li>Kamyar Ghassemipour</li>
		<li>Masatoshi Uehara </li>
		<li>Natasha	Jaques </li>
		<li>Ofir Nachum</li>
		<li>Ramki Gummadi</li>
	</ul>

	<ul style="width:25%; float:right; display: inline;">
		<li>Riashat	Islam</li>
		<li>Romain Laroche</li>
		<li>Romina	Abachi	</li>
		<li>Shangtong Zhang</li>
		<li>Taylor Killian</li>
		<li>Tengyang Xie</li>
		<li>Tianhe Yu</li>
		<li> Xinyang Geng </li>
		<li>Xue Bin	Peng</li>
		<li>Yao	Liu	</li>
		<li>Yevgen Chebotar</li>
		<li>Yi Su</li>
		<li>Ziyu Wang </li>
	</ul>
</div> -->

<div id="organizers" class="row">
<h2 style="float:left;">Organizers</h2>
<div class="break"></div>
<div style="text-align: left;">
{%- for person in site.data.organizers -%}
<div class="person">
  <img src="{{ person.image }}" height="170px" /><div style="height:12px;"></div>
   <a href="{{ person.url | relative_url }}">{{ person.name }}</a> <div style="height:4px;"></div>
   <span>{{ person.title | replace: '&', '<div style="height:4px;"></div>' }}</span>
</div>
{%- endfor -%}
</div>
</div>
<p> To contact the organizers, please send an email to <a href="mailto:darl.workshop@gmail.com">darl.workshop@gmail.com</a>. <p>


<p style="text-align:right">
  <small> Thanks to Rishabh Agarwal for providing this <a href="https://offline-rl-neurips.github.io/">template</a>. </small> </p>
